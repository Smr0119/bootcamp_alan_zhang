{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Starter — Stage 14: Deployment & Monitoring\n",
    "\n",
    "Use this template to draft your reflection and (optionally) sketch a dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reflection (200–300 words)\n",
    "\n",
    "**Risks if deployed:**\n",
    "Our model faces several risks in production. Data drift could occur when input patterns change over time, making predictions less accurate. Schema changes might break the input pipeline if new fields are added or removed. Missing data could increase beyond training levels, affecting model performance. System failures like API timeouts or memory issues could prevent predictions from being served.\n",
    "\n",
    "**Monitoring metrics across layers:**\n",
    "- **Data:** Monitor data freshness (max 30 minutes since last batch), null rate threshold at 15%, and schema hash changes. Track feature distribution drift using PSI > 0.25 as alert threshold.\n",
    "- **Model:** Watch rolling 7-day MAE staying below 0.05, and calibration error under 0.10. Set retraining trigger when performance drops below baseline for 3 consecutive days.\n",
    "- **System:** Monitor p95 latency staying under 200ms, error rate below 1%, and memory usage under 80%. Alert on-call engineer when thresholds are breached.\n",
    "- **Business:** Track approval rate staying within 2% of baseline, and bad rate remaining under 5%. Monthly business review checks these metrics.\n",
    "\n",
    "**Ownership & handoffs:**\n",
    "Platform team owns system monitoring and infrastructure alerts. Data team handles data quality checks and schema monitoring. Model team manages performance metrics and retraining decisions. Business analysts review KPIs weekly. Escalation follows: automated alerts → on-call engineer → team lead → manager for major issues. Runbook stored in team wiki with step-by-step recovery procedures.\n",
    "\n",
    "> Tip: Be specific (e.g., 'p95 latency > 250ms triggers on-call notification')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Optional: Dashboard Sketch\n",
    "\n",
    "**Dashboard Layout:**\n",
    "- Top row: System health (latency, error rate, throughput)\n",
    "- Second row: Data quality (freshness, null rates, drift metrics)\n",
    "- Third row: Model performance (accuracy, calibration, prediction distribution)\n",
    "- Bottom row: Business metrics (approval rates, volume trends)\n",
    "\n",
    "Each panel shows current value, 7-day trend, and red/yellow/green status indicators based on thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring = {\n",
    "    'data': ['freshness_minutes', 'null_rate', 'schema_hash', 'psi_drift'],\n",
    "    'model': ['rolling_mae', 'calibration_error', 'prediction_distribution'],\n",
    "    'system': ['p95_latency_ms', 'error_rate', 'memory_usage', 'throughput'],\n",
    "    'business': ['approval_rate', 'bad_rate', 'volume_trend']\n",
    "}\n",
    "monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
